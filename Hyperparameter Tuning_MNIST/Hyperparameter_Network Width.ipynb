{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 0.3153, Val Loss: 0.1605\n",
      "Epoch 2/50 - Loss: 0.1315, Val Loss: 0.1148\n",
      "Epoch 3/50 - Loss: 0.0913, Val Loss: 0.1054\n",
      "Epoch 4/50 - Loss: 0.0678, Val Loss: 0.0897\n",
      "Epoch 5/50 - Loss: 0.0536, Val Loss: 0.1001\n",
      "Epoch 6/50 - Loss: 0.0424, Val Loss: 0.0854\n",
      "Epoch 7/50 - Loss: 0.0364, Val Loss: 0.0817\n",
      "Epoch 8/50 - Loss: 0.0279, Val Loss: 0.1080\n",
      "Epoch 9/50 - Loss: 0.0230, Val Loss: 0.0896\n",
      "Epoch 10/50 - Loss: 0.0183, Val Loss: 0.0898\n",
      "Epoch 11/50 - Loss: 0.0125, Val Loss: 0.0951\n",
      "Epoch 12/50 - Loss: 0.0084, Val Loss: 0.0921\n",
      "Epoch 13/50 - Loss: 0.0073, Val Loss: 0.0870\n",
      "Epoch 14/50 - Loss: 0.0049, Val Loss: 0.0934\n",
      "Epoch 15/50 - Loss: 0.0029, Val Loss: 0.0922\n",
      "Epoch 16/50 - Loss: 0.0019, Val Loss: 0.1067\n",
      "Epoch 17/50 - Loss: 0.0013, Val Loss: 0.0933\n",
      "Epoch 18/50 - Loss: 0.0011, Val Loss: 0.0925\n",
      "Epoch 19/50 - Loss: 0.0008, Val Loss: 0.0934\n",
      "Epoch 20/50 - Loss: 0.0007, Val Loss: 0.0950\n",
      "Epoch 21/50 - Loss: 0.0007, Val Loss: 0.0948\n",
      "Epoch 22/50 - Loss: 0.0006, Val Loss: 0.0961\n",
      "Epoch 23/50 - Loss: 0.0005, Val Loss: 0.0961\n",
      "Epoch 24/50 - Loss: 0.0005, Val Loss: 0.0970\n",
      "Epoch 25/50 - Loss: 0.0005, Val Loss: 0.0978\n",
      "Epoch 26/50 - Loss: 0.0004, Val Loss: 0.0987\n",
      "Epoch 27/50 - Loss: 0.0004, Val Loss: 0.0990\n",
      "Epoch 28/50 - Loss: 0.0004, Val Loss: 0.0990\n",
      "Epoch 29/50 - Loss: 0.0004, Val Loss: 0.1005\n",
      "Epoch 30/50 - Loss: 0.0004, Val Loss: 0.1004\n",
      "Epoch 31/50 - Loss: 0.0003, Val Loss: 0.1008\n",
      "Epoch 32/50 - Loss: 0.0003, Val Loss: 0.1010\n",
      "Epoch 33/50 - Loss: 0.0003, Val Loss: 0.1015\n",
      "Epoch 34/50 - Loss: 0.0003, Val Loss: 0.1021\n",
      "Epoch 35/50 - Loss: 0.0003, Val Loss: 0.1023\n",
      "Epoch 36/50 - Loss: 0.0003, Val Loss: 0.1030\n",
      "Epoch 37/50 - Loss: 0.0003, Val Loss: 0.1037\n",
      "Epoch 38/50 - Loss: 0.0002, Val Loss: 0.1036\n",
      "Epoch 39/50 - Loss: 0.0002, Val Loss: 0.1040\n",
      "Epoch 40/50 - Loss: 0.0002, Val Loss: 0.1044\n",
      "Epoch 41/50 - Loss: 0.0002, Val Loss: 0.1050\n",
      "Epoch 42/50 - Loss: 0.0002, Val Loss: 0.1049\n",
      "Epoch 43/50 - Loss: 0.0002, Val Loss: 0.1051\n",
      "Epoch 44/50 - Loss: 0.0002, Val Loss: 0.1056\n",
      "Epoch 45/50 - Loss: 0.0002, Val Loss: 0.1059\n",
      "Epoch 46/50 - Loss: 0.0002, Val Loss: 0.1063\n",
      "Epoch 47/50 - Loss: 0.0002, Val Loss: 0.1063\n",
      "Epoch 48/50 - Loss: 0.0002, Val Loss: 0.1070\n",
      "Epoch 49/50 - Loss: 0.0002, Val Loss: 0.1072\n",
      "Epoch 50/50 - Loss: 0.0002, Val Loss: 0.1073\n",
      "Layer combination 1: Test Accuracy: 98.12%\n",
      "Epoch 1/50 - Loss: 0.2916, Val Loss: 0.1419\n",
      "Epoch 2/50 - Loss: 0.1159, Val Loss: 0.1040\n",
      "Epoch 3/50 - Loss: 0.0784, Val Loss: 0.0930\n",
      "Epoch 4/50 - Loss: 0.0578, Val Loss: 0.0895\n",
      "Epoch 5/50 - Loss: 0.0425, Val Loss: 0.0837\n",
      "Epoch 6/50 - Loss: 0.0331, Val Loss: 0.0757\n",
      "Epoch 7/50 - Loss: 0.0234, Val Loss: 0.0740\n",
      "Epoch 8/50 - Loss: 0.0184, Val Loss: 0.0845\n",
      "Epoch 9/50 - Loss: 0.0126, Val Loss: 0.0774\n",
      "Epoch 10/50 - Loss: 0.0085, Val Loss: 0.0816\n",
      "Epoch 11/50 - Loss: 0.0082, Val Loss: 0.0942\n",
      "Epoch 12/50 - Loss: 0.0046, Val Loss: 0.0785\n",
      "Epoch 13/50 - Loss: 0.0021, Val Loss: 0.0778\n",
      "Epoch 14/50 - Loss: 0.0015, Val Loss: 0.0777\n",
      "Epoch 15/50 - Loss: 0.0013, Val Loss: 0.0791\n",
      "Epoch 16/50 - Loss: 0.0011, Val Loss: 0.0790\n",
      "Epoch 17/50 - Loss: 0.0008, Val Loss: 0.0790\n",
      "Epoch 18/50 - Loss: 0.0007, Val Loss: 0.0803\n",
      "Epoch 19/50 - Loss: 0.0006, Val Loss: 0.0811\n",
      "Epoch 20/50 - Loss: 0.0006, Val Loss: 0.0814\n",
      "Epoch 21/50 - Loss: 0.0005, Val Loss: 0.0820\n",
      "Epoch 22/50 - Loss: 0.0005, Val Loss: 0.0823\n",
      "Epoch 23/50 - Loss: 0.0004, Val Loss: 0.0828\n",
      "Epoch 24/50 - Loss: 0.0004, Val Loss: 0.0830\n",
      "Epoch 25/50 - Loss: 0.0004, Val Loss: 0.0839\n",
      "Epoch 26/50 - Loss: 0.0004, Val Loss: 0.0840\n",
      "Epoch 27/50 - Loss: 0.0003, Val Loss: 0.0847\n",
      "Epoch 28/50 - Loss: 0.0003, Val Loss: 0.0846\n",
      "Epoch 29/50 - Loss: 0.0003, Val Loss: 0.0850\n",
      "Epoch 30/50 - Loss: 0.0003, Val Loss: 0.0857\n",
      "Epoch 31/50 - Loss: 0.0003, Val Loss: 0.0857\n",
      "Epoch 32/50 - Loss: 0.0003, Val Loss: 0.0861\n",
      "Epoch 33/50 - Loss: 0.0003, Val Loss: 0.0865\n",
      "Epoch 34/50 - Loss: 0.0003, Val Loss: 0.0867\n",
      "Epoch 35/50 - Loss: 0.0002, Val Loss: 0.0874\n",
      "Epoch 36/50 - Loss: 0.0002, Val Loss: 0.0874\n",
      "Epoch 37/50 - Loss: 0.0002, Val Loss: 0.0876\n",
      "Epoch 38/50 - Loss: 0.0002, Val Loss: 0.0876\n",
      "Epoch 39/50 - Loss: 0.0002, Val Loss: 0.0880\n",
      "Epoch 40/50 - Loss: 0.0002, Val Loss: 0.0884\n",
      "Epoch 41/50 - Loss: 0.0002, Val Loss: 0.0886\n",
      "Epoch 42/50 - Loss: 0.0002, Val Loss: 0.0889\n",
      "Epoch 43/50 - Loss: 0.0002, Val Loss: 0.0892\n",
      "Epoch 44/50 - Loss: 0.0002, Val Loss: 0.0892\n",
      "Epoch 45/50 - Loss: 0.0002, Val Loss: 0.0895\n",
      "Epoch 46/50 - Loss: 0.0002, Val Loss: 0.0897\n",
      "Epoch 47/50 - Loss: 0.0002, Val Loss: 0.0903\n",
      "Epoch 48/50 - Loss: 0.0002, Val Loss: 0.0904\n",
      "Epoch 49/50 - Loss: 0.0002, Val Loss: 0.0907\n",
      "Epoch 50/50 - Loss: 0.0001, Val Loss: 0.0908\n",
      "Layer combination 2: Test Accuracy: 98.23%\n",
      "Epoch 1/50 - Loss: 0.2769, Val Loss: 0.1331\n",
      "Epoch 2/50 - Loss: 0.1098, Val Loss: 0.0974\n",
      "Epoch 3/50 - Loss: 0.0715, Val Loss: 0.0888\n",
      "Epoch 4/50 - Loss: 0.0501, Val Loss: 0.0748\n",
      "Epoch 5/50 - Loss: 0.0364, Val Loss: 0.0849\n",
      "Epoch 6/50 - Loss: 0.0269, Val Loss: 0.0789\n",
      "Epoch 7/50 - Loss: 0.0189, Val Loss: 0.0768\n",
      "Epoch 8/50 - Loss: 0.0120, Val Loss: 0.0716\n",
      "Epoch 9/50 - Loss: 0.0078, Val Loss: 0.0710\n",
      "Epoch 10/50 - Loss: 0.0057, Val Loss: 0.0747\n",
      "Epoch 11/50 - Loss: 0.0032, Val Loss: 0.0711\n",
      "Epoch 12/50 - Loss: 0.0020, Val Loss: 0.0731\n",
      "Epoch 13/50 - Loss: 0.0013, Val Loss: 0.0747\n",
      "Epoch 14/50 - Loss: 0.0010, Val Loss: 0.0751\n",
      "Epoch 15/50 - Loss: 0.0009, Val Loss: 0.0758\n",
      "Epoch 16/50 - Loss: 0.0008, Val Loss: 0.0770\n",
      "Epoch 17/50 - Loss: 0.0007, Val Loss: 0.0766\n",
      "Epoch 18/50 - Loss: 0.0006, Val Loss: 0.0779\n",
      "Epoch 19/50 - Loss: 0.0006, Val Loss: 0.0786\n",
      "Epoch 20/50 - Loss: 0.0005, Val Loss: 0.0798\n",
      "Epoch 21/50 - Loss: 0.0005, Val Loss: 0.0797\n",
      "Epoch 22/50 - Loss: 0.0004, Val Loss: 0.0805\n",
      "Epoch 23/50 - Loss: 0.0004, Val Loss: 0.0808\n",
      "Epoch 24/50 - Loss: 0.0004, Val Loss: 0.0813\n",
      "Epoch 25/50 - Loss: 0.0004, Val Loss: 0.0818\n",
      "Epoch 26/50 - Loss: 0.0003, Val Loss: 0.0817\n",
      "Epoch 27/50 - Loss: 0.0003, Val Loss: 0.0820\n",
      "Epoch 28/50 - Loss: 0.0003, Val Loss: 0.0827\n",
      "Epoch 29/50 - Loss: 0.0003, Val Loss: 0.0828\n",
      "Epoch 30/50 - Loss: 0.0003, Val Loss: 0.0835\n",
      "Epoch 31/50 - Loss: 0.0003, Val Loss: 0.0835\n",
      "Epoch 32/50 - Loss: 0.0003, Val Loss: 0.0840\n",
      "Epoch 33/50 - Loss: 0.0002, Val Loss: 0.0843\n",
      "Epoch 34/50 - Loss: 0.0002, Val Loss: 0.0844\n",
      "Epoch 35/50 - Loss: 0.0002, Val Loss: 0.0850\n",
      "Epoch 36/50 - Loss: 0.0002, Val Loss: 0.0852\n",
      "Epoch 37/50 - Loss: 0.0002, Val Loss: 0.0855\n",
      "Epoch 38/50 - Loss: 0.0002, Val Loss: 0.0856\n",
      "Epoch 39/50 - Loss: 0.0002, Val Loss: 0.0859\n",
      "Epoch 40/50 - Loss: 0.0002, Val Loss: 0.0864\n",
      "Epoch 41/50 - Loss: 0.0002, Val Loss: 0.0865\n",
      "Epoch 42/50 - Loss: 0.0002, Val Loss: 0.0864\n",
      "Epoch 43/50 - Loss: 0.0002, Val Loss: 0.0867\n",
      "Epoch 44/50 - Loss: 0.0002, Val Loss: 0.0870\n",
      "Epoch 45/50 - Loss: 0.0002, Val Loss: 0.0874\n",
      "Epoch 46/50 - Loss: 0.0002, Val Loss: 0.0875\n",
      "Epoch 47/50 - Loss: 0.0002, Val Loss: 0.0880\n",
      "Epoch 48/50 - Loss: 0.0001, Val Loss: 0.0879\n",
      "Epoch 49/50 - Loss: 0.0001, Val Loss: 0.0880\n",
      "Epoch 50/50 - Loss: 0.0001, Val Loss: 0.0885\n",
      "Layer combination 3: Test Accuracy: 98.24%\n",
      "Epoch 1/50 - Loss: 0.3325, Val Loss: 0.1726\n",
      "Epoch 2/50 - Loss: 0.1551, Val Loss: 0.1282\n",
      "Epoch 3/50 - Loss: 0.1131, Val Loss: 0.1197\n",
      "Epoch 4/50 - Loss: 0.0906, Val Loss: 0.1095\n",
      "Epoch 5/50 - Loss: 0.0778, Val Loss: 0.1196\n",
      "Epoch 6/50 - Loss: 0.0645, Val Loss: 0.1236\n",
      "Epoch 7/50 - Loss: 0.0545, Val Loss: 0.1013\n",
      "Epoch 8/50 - Loss: 0.0472, Val Loss: 0.1140\n",
      "Epoch 9/50 - Loss: 0.0419, Val Loss: 0.1039\n",
      "Epoch 10/50 - Loss: 0.0345, Val Loss: 0.1082\n",
      "Epoch 11/50 - Loss: 0.0312, Val Loss: 0.1021\n",
      "Epoch 12/50 - Loss: 0.0290, Val Loss: 0.1155\n",
      "Epoch 13/50 - Loss: 0.0243, Val Loss: 0.1227\n",
      "Epoch 14/50 - Loss: 0.0198, Val Loss: 0.1190\n",
      "Epoch 15/50 - Loss: 0.0181, Val Loss: 0.1532\n",
      "Epoch 16/50 - Loss: 0.0168, Val Loss: 0.1148\n",
      "Epoch 17/50 - Loss: 0.0129, Val Loss: 0.1246\n",
      "Epoch 18/50 - Loss: 0.0113, Val Loss: 0.1899\n",
      "Epoch 19/50 - Loss: 0.0124, Val Loss: 0.1335\n",
      "Epoch 20/50 - Loss: 0.0106, Val Loss: 0.1270\n",
      "Epoch 21/50 - Loss: 0.0076, Val Loss: 0.1318\n",
      "Epoch 22/50 - Loss: 0.0066, Val Loss: 0.1368\n",
      "Epoch 23/50 - Loss: 0.0049, Val Loss: 0.1549\n",
      "Epoch 24/50 - Loss: 0.0044, Val Loss: 0.1360\n",
      "Epoch 25/50 - Loss: 0.0022, Val Loss: 0.1321\n",
      "Epoch 26/50 - Loss: 0.0010, Val Loss: 0.1374\n",
      "Epoch 27/50 - Loss: 0.0009, Val Loss: 0.1301\n",
      "Epoch 28/50 - Loss: 0.0006, Val Loss: 0.1307\n",
      "Epoch 29/50 - Loss: 0.0005, Val Loss: 0.1307\n",
      "Epoch 30/50 - Loss: 0.0005, Val Loss: 0.1321\n",
      "Epoch 31/50 - Loss: 0.0004, Val Loss: 0.1322\n",
      "Epoch 32/50 - Loss: 0.0004, Val Loss: 0.1335\n",
      "Epoch 33/50 - Loss: 0.0004, Val Loss: 0.1344\n",
      "Epoch 34/50 - Loss: 0.0003, Val Loss: 0.1350\n",
      "Epoch 35/50 - Loss: 0.0003, Val Loss: 0.1355\n",
      "Epoch 36/50 - Loss: 0.0003, Val Loss: 0.1354\n",
      "Epoch 37/50 - Loss: 0.0003, Val Loss: 0.1358\n",
      "Epoch 38/50 - Loss: 0.0003, Val Loss: 0.1372\n",
      "Epoch 39/50 - Loss: 0.0003, Val Loss: 0.1369\n",
      "Epoch 40/50 - Loss: 0.0003, Val Loss: 0.1376\n",
      "Epoch 41/50 - Loss: 0.0002, Val Loss: 0.1379\n",
      "Epoch 42/50 - Loss: 0.0002, Val Loss: 0.1384\n",
      "Epoch 43/50 - Loss: 0.0002, Val Loss: 0.1387\n",
      "Epoch 44/50 - Loss: 0.0002, Val Loss: 0.1392\n",
      "Epoch 45/50 - Loss: 0.0002, Val Loss: 0.1397\n",
      "Epoch 46/50 - Loss: 0.0002, Val Loss: 0.1401\n",
      "Epoch 47/50 - Loss: 0.0002, Val Loss: 0.1408\n",
      "Epoch 48/50 - Loss: 0.0002, Val Loss: 0.1408\n",
      "Epoch 49/50 - Loss: 0.0002, Val Loss: 0.1414\n",
      "Epoch 50/50 - Loss: 0.0002, Val Loss: 0.1416\n",
      "Layer combination 4: Test Accuracy: 97.65%\n",
      "Epoch 1/50 - Loss: 0.3213, Val Loss: 0.1743\n",
      "Epoch 2/50 - Loss: 0.1365, Val Loss: 0.1144\n",
      "Epoch 3/50 - Loss: 0.0940, Val Loss: 0.0984\n",
      "Epoch 4/50 - Loss: 0.0737, Val Loss: 0.0925\n",
      "Epoch 5/50 - Loss: 0.0574, Val Loss: 0.0971\n",
      "Epoch 6/50 - Loss: 0.0445, Val Loss: 0.0990\n",
      "Epoch 7/50 - Loss: 0.0386, Val Loss: 0.0959\n",
      "Epoch 8/50 - Loss: 0.0308, Val Loss: 0.0857\n",
      "Epoch 9/50 - Loss: 0.0242, Val Loss: 0.1110\n",
      "Epoch 10/50 - Loss: 0.0201, Val Loss: 0.0966\n",
      "Epoch 11/50 - Loss: 0.0156, Val Loss: 0.1119\n",
      "Epoch 12/50 - Loss: 0.0122, Val Loss: 0.1055\n",
      "Epoch 13/50 - Loss: 0.0090, Val Loss: 0.0912\n",
      "Epoch 14/50 - Loss: 0.0061, Val Loss: 0.1014\n",
      "Epoch 15/50 - Loss: 0.0039, Val Loss: 0.0912\n",
      "Epoch 16/50 - Loss: 0.0022, Val Loss: 0.0917\n",
      "Epoch 17/50 - Loss: 0.0015, Val Loss: 0.0924\n",
      "Epoch 18/50 - Loss: 0.0011, Val Loss: 0.0939\n",
      "Epoch 19/50 - Loss: 0.0009, Val Loss: 0.0956\n",
      "Epoch 20/50 - Loss: 0.0008, Val Loss: 0.0962\n",
      "Epoch 21/50 - Loss: 0.0006, Val Loss: 0.0978\n",
      "Epoch 22/50 - Loss: 0.0006, Val Loss: 0.0980\n",
      "Epoch 23/50 - Loss: 0.0005, Val Loss: 0.0985\n",
      "Epoch 24/50 - Loss: 0.0005, Val Loss: 0.0992\n",
      "Epoch 25/50 - Loss: 0.0005, Val Loss: 0.0999\n",
      "Epoch 26/50 - Loss: 0.0004, Val Loss: 0.1004\n",
      "Epoch 27/50 - Loss: 0.0004, Val Loss: 0.1010\n",
      "Epoch 28/50 - Loss: 0.0004, Val Loss: 0.1014\n",
      "Epoch 29/50 - Loss: 0.0004, Val Loss: 0.1020\n",
      "Epoch 30/50 - Loss: 0.0003, Val Loss: 0.1029\n",
      "Epoch 31/50 - Loss: 0.0003, Val Loss: 0.1030\n",
      "Epoch 32/50 - Loss: 0.0003, Val Loss: 0.1036\n",
      "Epoch 33/50 - Loss: 0.0003, Val Loss: 0.1038\n",
      "Epoch 34/50 - Loss: 0.0003, Val Loss: 0.1047\n",
      "Epoch 35/50 - Loss: 0.0003, Val Loss: 0.1049\n",
      "Epoch 36/50 - Loss: 0.0003, Val Loss: 0.1053\n",
      "Epoch 37/50 - Loss: 0.0002, Val Loss: 0.1058\n",
      "Epoch 38/50 - Loss: 0.0002, Val Loss: 0.1060\n",
      "Epoch 39/50 - Loss: 0.0002, Val Loss: 0.1066\n",
      "Epoch 40/50 - Loss: 0.0002, Val Loss: 0.1068\n",
      "Epoch 41/50 - Loss: 0.0002, Val Loss: 0.1073\n",
      "Epoch 42/50 - Loss: 0.0002, Val Loss: 0.1073\n",
      "Epoch 43/50 - Loss: 0.0002, Val Loss: 0.1080\n",
      "Epoch 44/50 - Loss: 0.0002, Val Loss: 0.1081\n",
      "Epoch 45/50 - Loss: 0.0002, Val Loss: 0.1088\n",
      "Epoch 46/50 - Loss: 0.0002, Val Loss: 0.1089\n",
      "Epoch 47/50 - Loss: 0.0002, Val Loss: 0.1092\n",
      "Epoch 48/50 - Loss: 0.0002, Val Loss: 0.1096\n",
      "Epoch 49/50 - Loss: 0.0002, Val Loss: 0.1098\n",
      "Epoch 50/50 - Loss: 0.0002, Val Loss: 0.1100\n",
      "Layer combination 5: Test Accuracy: 98.07%\n",
      "\n",
      "Best Layer Combination:\n",
      "Layers: [<mlp.Layer object at 0x0000020F2666F640>, <mlp.Layer object at 0x0000020F2666F6D0>, <mlp.Layer object at 0x0000020F2666F760>]\n",
      "Test Accuracy: 98.24%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import struct\n",
    "from array import array\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mlp import MultilayerPerceptron, Layer, CrossEntropy, Relu, Softmax\n",
    "\n",
    "# -------------------- MNIST Data Loader Class -------------------- #\n",
    "\n",
    "class MnistDataloader(object):\n",
    "    \"\"\"\n",
    "    A class to load and preprocess the MNIST dataset from IDX files.\n",
    "    \"\"\"\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        \"\"\"\n",
    "        Reads image and label data from IDX files.\n",
    "        \"\"\"\n",
    "        # Read labels\n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))  # Read magic number and size\n",
    "            if magic != 2049:\n",
    "                raise ValueError(f'Magic number mismatch, expected 2049, got {magic}')\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        # Read images\n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))  # Read metadata\n",
    "            if magic != 2051:\n",
    "                raise ValueError(f'Magic number mismatch, expected 2051, got {magic}')\n",
    "            image_data = array(\"B\", file.read())  # Read pixel data        \n",
    "        \n",
    "        # Reshape image data into (size, 28, 28)\n",
    "        images = [np.array(image_data[i * rows * cols:(i + 1) * rows * cols]).reshape(28, 28) for i in range(size)]\n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads training and test datasets.\n",
    "        \"\"\"\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train), (x_test, y_test)  \n",
    "\n",
    "# -------------------- Reading Dataset via MnistDataloader class -------------------- #\n",
    "\n",
    "data_folder = 'MNIST-data/'  # Change the directory accordingly\n",
    "\n",
    "# File paths for MNIST dataset\n",
    "training_images_filepath = os.path.join(data_folder, 'train-images.idx3-ubyte')\n",
    "training_labels_filepath = os.path.join(data_folder, 'train-labels.idx1-ubyte')\n",
    "test_images_filepath = os.path.join(data_folder, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = os.path.join(data_folder, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "# Loading MNIST dataset\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "# Converting lists to NumPy arrays and normalize pixel values\n",
    "x_train = np.array(x_train).reshape(-1, 784) / 255.0  # Normalize & flatten\n",
    "x_test = np.array(x_test).reshape(-1, 784) / 255.0\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Manual Train-Validation Split (80% train, 20% validation)\n",
    "split_idx = int(len(x_train) * 0.8)\n",
    "x_train, x_val = x_train[:split_idx], x_train[split_idx:]\n",
    "y_train, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "\n",
    "# Function to one-hot encode labels\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot[i][label] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Converting labels to one-hot encoding\n",
    "y_train = one_hot_encode(y_train)\n",
    "y_val = one_hot_encode(y_val)\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "# Function to display sample images\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images) / cols) + 1\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    index = 1\n",
    "    for img, title in zip(images, title_texts):\n",
    "        plt.subplot(rows, cols, index)\n",
    "        plt.imshow(img, cmap=plt.cm.gray)\n",
    "        plt.title(title, fontsize=15)\n",
    "        index += 1\n",
    "\n",
    "# -------------------- MLP Architecture for MNIST Dataset -------------------- #\n",
    "\n",
    "# Define different layer combinations\n",
    "layer_combinations = [\n",
    "    [Layer(784, 128, Relu(), dropout_rate=0.0), Layer(128, 64, Relu(), dropout_rate=0.0), Layer(64, 10, Softmax())],\n",
    "    [Layer(784, 256, Relu(), dropout_rate=0.0), Layer(256, 128, Relu(), dropout_rate=0.0), Layer(128, 10, Softmax())],\n",
    "    [Layer(784, 512, Relu(), dropout_rate=0.0), Layer(512, 256, Relu(), dropout_rate=0.0), Layer(256, 10, Softmax())],\n",
    "    [Layer(784, 64, Relu(), dropout_rate=0.0), Layer(64, 32, Relu(), dropout_rate=0.0), Layer(32, 10, Softmax())],\n",
    "    [Layer(784, 128, Relu(), dropout_rate=0.0), Layer(128, 32, Relu(), dropout_rate=0.0), Layer(32, 10, Softmax())]\n",
    "]\n",
    "\n",
    "# Store results of test accuracies\n",
    "results = {}\n",
    "\n",
    "# Loop through different layer combinations\n",
    "for idx, layers in enumerate(layer_combinations):\n",
    "    # Defining the MLP model with the given layers\n",
    "    mlp = MultilayerPerceptron(layers)\n",
    "\n",
    "    # Defining loss function\n",
    "    loss_function = CrossEntropy()\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses = mlp.train(\n",
    "        x_train, y_train, x_val, y_val,\n",
    "        loss_function, learning_rate=0.001,\n",
    "        batch_size=64, epochs=50, optimizer='vanilla', momentum=0.7\n",
    "    )\n",
    "\n",
    "    # Evaluating model on test set\n",
    "    y_pred = mlp.forward(x_test)\n",
    "    test_accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "    # Store the result\n",
    "    results[idx] = {\n",
    "        \"layers\": layers,\n",
    "        \"test_accuracy\": test_accuracy\n",
    "    }\n",
    "\n",
    "    print(f\"Layer combination {idx+1}: Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Find the best layer combination based on test accuracy\n",
    "best_combination_idx = max(results, key=lambda x: results[x][\"test_accuracy\"])\n",
    "best_combination = results[best_combination_idx]\n",
    "\n",
    "print(\"\\nBest Layer Combination:\")\n",
    "print(f\"Layers: {best_combination['layers']}\")\n",
    "print(f\"Test Accuracy: {best_combination['test_accuracy'] * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
